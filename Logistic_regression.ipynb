{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt \n",
    "import cupy as cp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=data[\"data\"][:,3:]\n",
    "Y=(data[\"target\"]==2).astype(np.int32)\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "Logistic regression is named for the function used at the core of the method, the [logistic function](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "\n",
    "The logistic function, also called the **Sigmoid function** was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. Itâ€™s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$e$ is the base of the natural logarithms and $x$ is value that you want to transform via the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression equation has a very simiar representation like linear regression. The difference is that the output value being modelled is binary in nature.\n",
    "\n",
    "$$\\hat{y}=\\frac{e^{\\beta_0+\\beta_1x_1}}{1+\\beta_0+\\beta_1x_1}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\hat{y}=\\frac{1.0}{1.0+e^{-\\beta_0-\\beta_1x_1}}$$\n",
    "\n",
    "$\\beta_0$ is the intecept term\n",
    "\n",
    "$\\beta_1$ is the coefficient for $x_1$\n",
    "\n",
    "$\\hat{y}$ is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Stochastic Gradient Descent\n",
    "\n",
    "Logistic Regression uses gradient descent to update the coefficients.\n",
    "\n",
    "Each gradient descent iteration, the coefficients are updated using the equation:\n",
    "\n",
    "$$\\beta=\\beta+\\textrm{learning rate}\\times (y-\\hat{y}) \\times \\hat{y} \\times (1-\\hat{y}) \\times x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "def loss(yh,y):\n",
    "    return yh-y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_regression():\n",
    "    def __init__(self,shapes,lr=0.01):\n",
    "        self.w=np.random.randn(shapes[1],1)\n",
    "        self.b=np.random.randn(shapes[0],1)\n",
    "        self.lr=lr\n",
    "    def fit(self,x,y):\n",
    "        k=np.dot(x,self.w)+self.b\n",
    "        yh=sigmoid(k)\n",
    "        l=loss(yh,y)\n",
    "        dw=np.mean(l.dot(x))\n",
    "        db=2*np.sum(yh-y)\n",
    "        self.w-=self.lr*dw\n",
    "        self.b-=self.lr*db\n",
    "        return l\n",
    "    def predict(self,x):\n",
    "        k=np.dot(x,self.w)\n",
    "        yh=sigmoid(k)\n",
    "        return yh\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43812422  0.56187578  0.56187578 ...  0.56187578 -0.43812422\n",
      "   0.56187578]\n",
      " [-0.25459919  0.74540081  0.74540081 ...  0.74540081 -0.25459919\n",
      "   0.74540081]\n",
      " [-0.96365582  0.03634418  0.03634418 ...  0.03634418 -0.96365582\n",
      "   0.03634418]\n",
      " ...\n",
      " [-0.32239167  0.67760833  0.67760833 ...  0.67760833 -0.32239167\n",
      "   0.67760833]\n",
      " [-0.48496287  0.51503713  0.51503713 ...  0.51503713 -0.48496287\n",
      "   0.51503713]\n",
      " [-0.40141262  0.59858738  0.59858738 ...  0.59858738 -0.40141262\n",
      "   0.59858738]]\n",
      "[[-9.98921225e-01  1.07877547e-03  1.07877547e-03 ...  1.07877547e-03\n",
      "  -9.98921225e-01  1.07877547e-03]\n",
      " [-9.97569428e-01  2.43057221e-03  2.43057221e-03 ...  2.43057221e-03\n",
      "  -9.97569428e-01  2.43057221e-03]\n",
      " [-9.99968597e-01  3.14025402e-05  3.14025402e-05 ...  3.14025402e-05\n",
      "  -9.99968597e-01  3.14025402e-05]\n",
      " ...\n",
      " [-9.98243589e-01  1.75641091e-03  1.75641091e-03 ...  1.75641091e-03\n",
      "  -9.98243589e-01  1.75641091e-03]\n",
      " [-9.99106970e-01  8.93029662e-04  8.93029662e-04 ...  8.93029662e-04\n",
      "  -9.99106970e-01  8.93029662e-04]\n",
      " [-9.98760546e-01  1.23945416e-03  1.23945416e-03 ...  1.23945416e-03\n",
      "  -9.98760546e-01  1.23945416e-03]]\n"
     ]
    }
   ],
   "source": [
    "lg=Logistic_regression(shapes=(135,1),lr=0.001)\n",
    "for _ in range(2):\n",
    "    l=lg.fit(x_train,y_train)\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6116469 ]\n",
      " [0.51135405]\n",
      " [0.50567776]\n",
      " [0.60623852]\n",
      " [0.58986133]\n",
      " [0.58435572]\n",
      " [0.50567776]\n",
      " [0.62770323]\n",
      " [0.58435572]\n",
      " [0.51135405]\n",
      " [0.60080398]\n",
      " [0.62770323]\n",
      " [0.61702792]\n",
      " [0.60080398]\n",
      " [0.56771737]]\n",
      "[1 0 0 1 0 0 0 1 0 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(lg.predict(x_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR = LogisticRegression(C=1.0, tol=0.0001)\n",
    "#0.1<c<0.5 \n",
    "# 1<c<10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_LR.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 0 1 0 0 1 1 1 1 0]\n",
      "[[0.20199685 0.79800315]\n",
      " [0.99761177 0.00238823]\n",
      " [0.9984163  0.0015837 ]\n",
      " [0.27642441 0.72357559]\n",
      " [0.56770923 0.43229077]\n",
      " [0.664654   0.335346  ]\n",
      " [0.9984163  0.0015837 ]\n",
      " [0.06858449 0.93141551]\n",
      " [0.664654   0.335346  ]\n",
      " [0.99761177 0.00238823]\n",
      " [0.36570787 0.63429213]\n",
      " [0.06858449 0.93141551]\n",
      " [0.14363107 0.85636893]\n",
      " [0.36570787 0.63429213]\n",
      " [0.87201374 0.12798626]]\n",
      "[1 0 0 1 0 0 0 1 0 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(clf_LR.predict(x_test))\n",
    "print(clf_LR.predict_proba(x_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Penalties \n",
    "Using lasso and Tikhonov functions to regulize intersect and slope margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 0 1 0 0 1 1 1 1 0]\n",
      "[[0.20199685 0.79800315]\n",
      " [0.99761177 0.00238823]\n",
      " [0.9984163  0.0015837 ]\n",
      " [0.27642441 0.72357559]\n",
      " [0.56770923 0.43229077]\n",
      " [0.664654   0.335346  ]\n",
      " [0.9984163  0.0015837 ]\n",
      " [0.06858449 0.93141551]\n",
      " [0.664654   0.335346  ]\n",
      " [0.99761177 0.00238823]\n",
      " [0.36570787 0.63429213]\n",
      " [0.06858449 0.93141551]\n",
      " [0.14363107 0.85636893]\n",
      " [0.36570787 0.63429213]\n",
      " [0.87201374 0.12798626]]\n",
      "[1 0 0 1 0 0 0 1 0 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "clf_LR = LogisticRegression(C=1.0,penalty=\"l2\",tol=0.0001)\n",
    "clf_LR.fit(x_train,y_train)\n",
    "print(clf_LR.predict(x_test))\n",
    "print(clf_LR.predict_proba(x_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Multiclass classification \n",
    "\n",
    "Softmax \n",
    "\n",
    "$$\\frac{e^{x}} {\\sum_{i=1}^{N}e^{xi}}$$\n",
    "\n",
    "$e$ is the base of the natural logarithms and $x$ is value that you want to transform via the logistic function.\n",
    "$N$ is the number of classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
